# VLM Visualization Literacy Assessment

This repository contains the implementation and evaluation framework for assessing visualization literacy capabilities of Visual Language Models (VLMs) using standardized tests VLAT and CALVI. The study provides a comprehensive comparison of four state-of-the-art VLMs' abilities to interpret, reason about, and critically analyze data visualizations.

## ğŸ¯ Project Overview

The project evaluates VLMs through:
- Visualization Literacy Assessment Test (VLAT) - 53 multiple-choice items across 12 visualization types
- Critical thinking Assessment for Literacy in Visualization (CALVI) - 45 items focused on misleading visualization elements
- 10 randomized evaluation runs per model to ensure robust results

## ğŸ¤– Models Evaluated

| Model | Version | Provider |
|-------|----------|----------|
| GPT-4 Vision | GPT-4o | OpenAI |
| Claude | 3.5 Sonnet | Anthropic |
| Gemini | 1.5 Pro | Google |
| Llama | 3.2-vision | Meta |

All models are configured with:
- Temperature: 0
- Max tokens: 300

## ğŸ“ Repository Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ VLAT/                 # VLAT test images and questions
â”‚   â””â”€â”€ CALVI/                # CALVI test images and questions
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ gpt4_evaluation.ipynb        # GPT-4 Vision evaluation notebook
â”‚   â”œâ”€â”€ claude_evaluation.ipynb      # Claude evaluation notebook
â”‚   â”œâ”€â”€ gemini_evaluation.ipynb      # Gemini evaluation notebook
â”‚   â”œâ”€â”€ llama_evaluation.ipynb       # Llama evaluation notebook
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ VLAT_prompt.txt      # Standardized VLAT assessment prompt
â”‚   â””â”€â”€ CALVI_prompt.txt     # Standardized CALVI assessment prompt
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ raw/                 # Raw model responses
```

## ğŸš€ Getting Started

1. Clone the repository:
```bash
git clone https://github.com/washuvis/VisLit-VLM-Eval.git
```

2. Install required dependencies:
```bash
pip install -r requirements.txt
```

3. Configure API keys:
   - Add your API keys for each VLM provider

4. Run evaluations:
   - Navigate to the `scripts` directory
   - Execute evaluation notebooks for each model

## ğŸ“§ Contact

For questions or feedback, please contact Saugat Pandey (p.saugat@wustl.edu) or Dr. Alvitta Ottley (alvitta@wustl.edu)