{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "import traceback\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import mimetypes\n",
    "from typing import List, Dict, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6048b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='Add your API key here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RUNS = 10\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 10  # seconds\n",
    "TEMPERATURE = 0\n",
    "MAX_TOKENS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLAT_PROMPT = \"\"\"I am about to show you an image and ask you a multiple choice question about that image. \n",
    "Please structure your response in the following format:\n",
    "Answer: [Enter the exact text of your chosen option]\n",
    "Explanation: [Provide your reasoning]\n",
    "Select the BEST answer, based only on the chart and not external knowledge. DO NOT GUESS.\n",
    "If you are not sure about your answer or your answer is based on a guess, select \"Omit\".\n",
    "Choose your answer ONLY from the provided options.\"\"\"\n",
    "\n",
    "CALVI_PROMPT = \"\"\"I am about to show you an image and ask you a multiple choice question about that image. \n",
    "Please structure your response in the following format:\n",
    "Answer: [Enter the exact text of your chosen option(s)]\n",
    "Explanation: [Provide your reasoning]\n",
    "Select the BEST answer(s), based only on the chart and not external knowledge.\n",
    "Choose your answer(s) ONLY from the provided options.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d851ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data['questions']\n",
    "\n",
    "def get_image_mime_type(image_path):\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    return mime_type if mime_type else 'image/png'\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def query_gpt_with_retry(prompt, image_path, temperature=TEMPERATURE, max_tokens=MAX_TOKENS):\n",
    "    base64_image = encode_image(image_path)\n",
    "    mime_type = get_image_mime_type(image_path)\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:{mime_type};base64,{base64_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred (Attempt {attempt + 1}/{MAX_RETRIES}): {str(e)}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this question.\")\n",
    "                return \"Error: Max retries reached\"\n",
    "\n",
    "def extract_answer_and_explanation(gpt_answer, options):\n",
    "    parts = gpt_answer.split('Why:', 1)\n",
    "    chosen_option = parts[0].strip()\n",
    "    explanation = parts[1].strip() if len(parts) > 1 else \"No explanation provided\"\n",
    "    \n",
    "    chosen_option = chosen_option.strip('*').strip()\n",
    "    \n",
    "    if chosen_option.isdigit():\n",
    "        index = int(chosen_option) - 1\n",
    "        if 0 <= index < len(options):\n",
    "            return [options[index]], explanation\n",
    "    \n",
    "    matches = []\n",
    "    for opt in options:\n",
    "        if opt.lower() in chosen_option.lower():\n",
    "            matches.append(opt)\n",
    "        elif any(word.lower() in chosen_option.lower() for word in opt.split()):\n",
    "            matches.append(opt)\n",
    "    \n",
    "    return matches if matches else [chosen_option], explanation\n",
    "\n",
    "def evaluate_answer(correct_answer, gpt_answer):\n",
    "    correct_answers = set(answer.strip().lower() for answer in correct_answer.split(','))\n",
    "    gpt_answers = set(answer.strip().lower() for answer in gpt_answer)\n",
    "    return bool(correct_answers & gpt_answers)\n",
    "\n",
    "def extract_answer_and_explanation(gpt_answer: str, options: List[str]) -> Tuple[List[str], str]:\n",
    "    \"\"\"\n",
    "    Extract answer and explanation from GPT's response using robust pattern matching.\n",
    "    \"\"\"\n",
    "    # Handle empty response\n",
    "    if not gpt_answer:\n",
    "        return [], \"No response provided\"\n",
    "    \n",
    "    # Define markers for answer and explanation sections\n",
    "    answer_markers = [\n",
    "        \"Answer:\", \n",
    "        \"answer:\",\n",
    "        \"The best answer based on the information provided in the image is:\",\n",
    "        \"The best answer based on the chart is:\",\n",
    "        \"The best answer is:\"\n",
    "    ]\n",
    "    \n",
    "    explanation_markers = [\n",
    "        \"Explanation:\",\n",
    "        \"explanation:\",\n",
    "        \"Why:\",\n",
    "        \"why:\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize variables\n",
    "    chosen_option = \"\"\n",
    "    explanation = \"\"\n",
    "    \n",
    "    # Convert to lowercase for case-insensitive searching\n",
    "    lower_response = gpt_answer.lower()\n",
    "    \n",
    "    # Find the start of the answer section\n",
    "    answer_start = -1\n",
    "    answer_marker_used = \"\"\n",
    "    for marker in answer_markers:\n",
    "        if marker.lower() in lower_response:\n",
    "            answer_start = lower_response.index(marker.lower()) + len(marker)\n",
    "            answer_marker_used = marker\n",
    "            break\n",
    "    \n",
    "    # Find the start of the explanation section\n",
    "    explanation_start = -1\n",
    "    explanation_marker_used = \"\"\n",
    "    for marker in explanation_markers:\n",
    "        if marker.lower() in lower_response:\n",
    "            explanation_start = lower_response.index(marker.lower())\n",
    "            explanation_marker_used = marker\n",
    "            break\n",
    "    \n",
    "    # Extract answer and explanation\n",
    "    if answer_start >= 0:\n",
    "        if explanation_start >= 0:\n",
    "            # We have both answer and explanation\n",
    "            chosen_option = gpt_answer[answer_start:explanation_start].strip()\n",
    "            explanation = gpt_answer[explanation_start + len(explanation_marker_used):].strip()\n",
    "        else:\n",
    "            # We only have answer\n",
    "            chosen_option = gpt_answer[answer_start:].strip()\n",
    "    else:\n",
    "        # Fallback: use the first line as answer if it's not too long\n",
    "        first_line = gpt_answer.split('\\n')[0].strip()\n",
    "        if len(first_line) < 100:  # Arbitrary length check to avoid using explanations as answers\n",
    "            chosen_option = first_line\n",
    "    \n",
    "    # Clean up the chosen option\n",
    "    chosen_option = chosen_option.strip()\n",
    "    \n",
    "    # Remove numbering if present (e.g., \"3) Answer\" -> \"Answer\")\n",
    "    if chosen_option and chosen_option[0].isdigit() and ') ' in chosen_option:\n",
    "        chosen_option = chosen_option.split(') ', 1)[1].strip()\n",
    "    \n",
    "    # Remove quotes if present\n",
    "    chosen_option = chosen_option.strip('\"\\'')\n",
    "    \n",
    "    # Match with provided options\n",
    "    matches = []\n",
    "    for opt in options:\n",
    "        # Exact match (case-insensitive)\n",
    "        if opt.lower().strip() == chosen_option.lower().strip():\n",
    "            return [opt], explanation\n",
    "    \n",
    "    # If no exact match, try partial matches\n",
    "    for opt in options:\n",
    "        # Check if the option is contained within the chosen answer\n",
    "        if opt.lower().strip() in chosen_option.lower():\n",
    "            matches.append(opt)\n",
    "    \n",
    "    # If we found matches, return them\n",
    "    if matches:\n",
    "        return [matches[0]], explanation\n",
    "    \n",
    "    # Handle \"Omit\" case for VLAT\n",
    "    if \"omit\" in chosen_option.lower():\n",
    "        return [\"Omit\"], explanation\n",
    "    \n",
    "    # If no matches but we have a chosen_option, return it\n",
    "    if chosen_option and len(chosen_option) > 1:\n",
    "        return [chosen_option], explanation\n",
    "    \n",
    "    return [\"No valid answer extracted\"], explanation\n",
    "\n",
    "def generate_structured_prompt(question: Dict, test_type: str, base_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a structured prompt for either VLAT or CALVI questions.\n",
    "    \"\"\"\n",
    "    options = question['options'].copy()\n",
    "    \n",
    "    question_prompt = f\"Question: {question['question']}\\n\\nOptions:\\n\"\n",
    "    for i, option in enumerate(options, 1):\n",
    "        question_prompt += f\"{i}) {option}\\n\"\n",
    "    \n",
    "    question_prompt += f\"\\n{base_prompt}\"\n",
    "    \n",
    "    return question_prompt\n",
    "\n",
    "def evaluate_visualization_literacy(test_name: str, questions: List[Dict], prompt: str,\n",
    "                                 randomize_options: bool = False,\n",
    "                                 randomize_questions: bool = False) -> List[Dict]:\n",
    "    results = []\n",
    "    working_questions = questions.copy()\n",
    "    \n",
    "    if randomize_questions:\n",
    "        random.shuffle(working_questions)\n",
    "    \n",
    "    for idx, question in enumerate(working_questions, 1):\n",
    "        print(f\"\\nProcessing {test_name} question {idx}/{len(working_questions)}\")\n",
    "        print(f\"Conditions: Options {'Randomized' if randomize_options else 'Not Randomized'}, \"\n",
    "              f\"Questions {'Randomized' if randomize_questions else 'Not Randomized'}\")\n",
    "        \n",
    "        options = question['options'].copy()\n",
    "        if randomize_options:\n",
    "            random.shuffle(options)\n",
    "        \n",
    "        question_prompt = generate_structured_prompt(\n",
    "            question={\"question\": question['question'], \"options\": options},\n",
    "            test_type=test_name,\n",
    "            base_prompt=prompt\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        full_gpt_answer = query_gpt_with_retry(question_prompt, question['image_path'])\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "        print(f\"Raw GPT response: {full_gpt_answer}\")\n",
    "        \n",
    "        gpt_answer, explanation = extract_answer_and_explanation(full_gpt_answer, options)\n",
    "        is_correct = evaluate_answer(question['correct_answer'], gpt_answer)\n",
    "        \n",
    "        results.append({\n",
    "            'test_name': test_name,\n",
    "            'question': question['question'],\n",
    "            'options': ', '.join(options),\n",
    "            'correct_answer': question['correct_answer'],\n",
    "            'gpt_answer': ', '.join(gpt_answer),\n",
    "            'explanation': explanation,\n",
    "            'raw_response': full_gpt_answer,\n",
    "            'Task': question.get('Task', ''),\n",
    "            'Chart_type': question.get('Chart_type', ''),\n",
    "            'Misleader': question.get('Misleader', ''),\n",
    "            'wrong_due_to_misleader': question.get('wrong_due_to_misleader', ''),\n",
    "            'is_correct': is_correct,\n",
    "            'randomized_options': randomize_options,\n",
    "            'randomized_questions': randomize_questions,\n",
    "            'image_path': question['image_path'],\n",
    "            'time_taken': time_taken\n",
    "        })\n",
    "        \n",
    "        print(f\"Question: {question['question']}\")\n",
    "        print(f\"Options: {', '.join(options)}\")\n",
    "        print(f\"GPT's answer: {', '.join(gpt_answer)}\")\n",
    "        print(f\"GPT's explanation: {explanation}\")\n",
    "        print(f\"Correct answer: {question['correct_answer']}\")\n",
    "        print(f\"Result: {'Correct' if is_correct else 'Incorrect'}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_experiment(test_name: str, file_path: str, prompt: str):\n",
    "    print(f\"\\nStarting {test_name} experiment...\")\n",
    "    questions = load_questions(file_path)\n",
    "    \n",
    "    conditions = [\n",
    "         (False, False, \"No_Randomization\"),\n",
    "         (True, False, \"Randomized_Options\"),\n",
    "        (False, True, \"Randomized_Questions\"),\n",
    "        (True, True, \"Both_Randomized\")\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for randomize_options, randomize_questions, condition_name in conditions:\n",
    "        print(f\"\\n=== Running {test_name} - {condition_name} ===\")\n",
    "        \n",
    "        for run in range(1, NUM_RUNS + 1):\n",
    "            print(f\"\\n--- Run {run}/{NUM_RUNS} ---\")\n",
    "            results = evaluate_visualization_literacy(\n",
    "                test_name,\n",
    "                questions, \n",
    "                prompt,\n",
    "                randomize_options=randomize_options,\n",
    "                randomize_questions=randomize_questions\n",
    "            )\n",
    "            \n",
    "            # Add condition information to results\n",
    "            for result in results:\n",
    "                result['condition'] = condition_name\n",
    "                result['run'] = run\n",
    "            \n",
    "            all_results.extend(results)\n",
    "            \n",
    "            # Save individual run results\n",
    "            df_run = pd.DataFrame(results)\n",
    "            df_run.to_csv(f'gpt_{test_name.lower()}_{condition_name}_run_{run}.csv', index=False)\n",
    "            \n",
    "            # Calculate and print run score\n",
    "            score = (df_run['is_correct'].sum() / len(df_run)) * 100\n",
    "            print(f\"\\nScore for {condition_name} Run {run}: {score:.2f}%\")\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Calculate and print overall statistics\n",
    "    print(\"\\n=== Overall Results ===\")\n",
    "    for condition in combined_df['condition'].unique():\n",
    "        condition_df = combined_df[combined_df['condition'] == condition]\n",
    "        print(f\"\\n{condition}:\")\n",
    "        print(f\"Mean accuracy: {condition_df['is_correct'].mean() * 100:.2f}%\")\n",
    "        print(f\"Best question accuracy: {condition_df.groupby('question')['is_correct'].mean().max() * 100:.2f}%\")\n",
    "        print(f\"Worst question accuracy: {condition_df.groupby('question')['is_correct'].mean().min() * 100:.2f}%\")\n",
    "        print(f\"Average time per question: {condition_df['time_taken'].mean():.2f} seconds\")\n",
    "        print(f\"Fastest question: {condition_df['time_taken'].min():.2f} seconds\")\n",
    "        print(f\"Slowest question: {condition_df['time_taken'].max():.2f} seconds\")\n",
    "    \n",
    "    # Statistics by various dimensions\n",
    "    print(\"\\nAverage Statistics by Task:\")\n",
    "    print(combined_df.groupby(['Task', 'condition'])['is_correct'].mean().unstack())\n",
    "    \n",
    "    print(\"\\nAverage Statistics by Chart Type:\")\n",
    "    print(combined_df.groupby(['Chart_type', 'condition'])['is_correct'].mean().unstack())\n",
    "    \n",
    "    if 'Misleader' in combined_df.columns:\n",
    "        print(\"\\nAverage Statistics by Misleader Type:\")\n",
    "        print(combined_df.groupby(['Misleader', 'condition'])['is_correct'].mean().unstack())\n",
    "    \n",
    "    # Save combined results\n",
    "    combined_df.to_csv(f'gpt_{test_name.lower()}_all_results.csv', index=False)\n",
    "    print(f\"\\nCombined results saved to gpt_{test_name.lower()}_all_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_experiment(\"VLAT\", \"vlat_skip.json\", VLAT_PROMPT)\n",
    "        run_experiment(\"CALVI\", \"calvi.json\", CALVI_PROMPT)\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642cd456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233b44b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
