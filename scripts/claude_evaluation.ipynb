{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "import traceback\n",
    "import anthropic\n",
    "import base64\n",
    "import os\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key='Add_your_api_key_here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RUNS = 10\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 10  # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLAT_PROMPT = \"\"\"I am about to show you an image and ask you a multiple choice question about that image. \n",
    "Please structure your response in the following format:\n",
    "Answer: [Enter the exact text of your chosen option]\n",
    "Explanation: [Provide your reasoning]\n",
    "Select the BEST answer, based only on the chart and not external knowledge. DO NOT GUESS.\n",
    "If you are not sure about your answer or your answer is based on a guess, select \"Omit\".\n",
    "Choose your answer ONLY from the provided options.\"\"\"\n",
    "\n",
    "CALVI_PROMPT = \"\"\"I am about to show you an image and ask you a multiple choice question about that image. \n",
    "Please structure your response in the following format:\n",
    "Answer: [Enter the exact text of your chosen option(s)]\n",
    "Explanation: [Provide your reasoning]\n",
    "Select the BEST answer, based only on the chart and not external knowledge.\n",
    "Choose your answer ONLY from the provided options.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ef80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions(file_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data['questions']\n",
    "\n",
    "def get_image_mime_type(image_path: str) -> str:\n",
    "    extension = os.path.splitext(image_path)[1].lower()\n",
    "    if extension in ['.jpg', '.jpeg']:\n",
    "        return 'image/jpeg'\n",
    "    elif extension == '.png':\n",
    "        return 'image/png'\n",
    "    elif extension == '.gif':\n",
    "        return 'image/gif'\n",
    "    elif extension == '.webp':\n",
    "        return 'image/webp'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image format: {extension}\")\n",
    "\n",
    "def encode_image(image_path: str) -> str:\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def query_claude_with_retry(prompt: str, image_path: str) -> str:\n",
    "    base64_image = encode_image(image_path)\n",
    "    mime_type = get_image_mime_type(image_path)\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20240620\",\n",
    "                max_tokens=300,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": prompt\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image\",\n",
    "                                \"source\": {\n",
    "                                    \"type\": \"base64\",\n",
    "                                    \"media_type\": mime_type,\n",
    "                                    \"data\": base64_image\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred (Attempt {attempt + 1}/{MAX_RETRIES}): {str(e)}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this question.\")\n",
    "                return \"Error: Max retries reached\"\n",
    "\n",
    "def extract_answer_and_explanation(claude_answer: str, options: List[str]) -> tuple:\n",
    "    # Handle empty response\n",
    "    if not claude_answer:\n",
    "        return [], \"No response provided\"\n",
    "    \n",
    "    # First try to find a clearly marked answer section\n",
    "    answer_markers = [\n",
    "        \"Answer:\", \n",
    "        \"answer:\",\n",
    "        \"The best answer based on the information provided in the image is:\",\n",
    "        \"The best answer based on the chart is:\",\n",
    "        \"The best answer is:\"\n",
    "    ]\n",
    "    \n",
    "    explanation_markers = [\n",
    "        \"Explanation:\",\n",
    "        \"explanation:\",\n",
    "        \"Why:\",\n",
    "        \"why:\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize variables\n",
    "    chosen_option = \"\"\n",
    "    explanation = \"\"\n",
    "    \n",
    "    # Try to find the answer section\n",
    "    lower_response = claude_answer.lower()\n",
    "    \n",
    "    # Find the start of the answer section\n",
    "    answer_start = -1\n",
    "    answer_marker_used = \"\"\n",
    "    for marker in answer_markers:\n",
    "        if marker.lower() in lower_response:\n",
    "            answer_start = lower_response.index(marker.lower()) + len(marker)\n",
    "            answer_marker_used = marker\n",
    "            break\n",
    "    \n",
    "    # Find the start of the explanation section\n",
    "    explanation_start = -1\n",
    "    explanation_marker_used = \"\"\n",
    "    for marker in explanation_markers:\n",
    "        if marker.lower() in lower_response:\n",
    "            explanation_start = lower_response.index(marker.lower())\n",
    "            explanation_marker_used = marker\n",
    "            break\n",
    "    \n",
    "    # Extract answer and explanation\n",
    "    if answer_start >= 0:\n",
    "        if explanation_start >= 0:\n",
    "            # We have both answer and explanation\n",
    "            chosen_option = claude_answer[answer_start:explanation_start].strip()\n",
    "            explanation = claude_answer[explanation_start + len(explanation_marker_used):].strip()\n",
    "        else:\n",
    "            # We only have answer\n",
    "            chosen_option = claude_answer[answer_start:].strip()\n",
    "    else:\n",
    "        # Fallback: use the first line as answer if it's not too long\n",
    "        first_line = claude_answer.split('\\n')[0].strip()\n",
    "        if len(first_line) < 100:  # Arbitrary length check to avoid using explanations as answers\n",
    "            chosen_option = first_line\n",
    "    \n",
    "    # Clean up the chosen option\n",
    "    chosen_option = chosen_option.strip()\n",
    "    \n",
    "    # Remove numbering if present (e.g., \"3) Answer\" -> \"Answer\")\n",
    "    if chosen_option and chosen_option[0].isdigit() and ') ' in chosen_option:\n",
    "        chosen_option = chosen_option.split(') ', 1)[1].strip()\n",
    "    \n",
    "    # Remove quotes if present\n",
    "    chosen_option = chosen_option.strip('\"\\'')\n",
    "    \n",
    "    # Match with provided options\n",
    "    matches = []\n",
    "    for opt in options:\n",
    "        # Exact match (case-insensitive)\n",
    "        if opt.lower().strip() == chosen_option.lower().strip():\n",
    "            return [opt], explanation\n",
    "    \n",
    "    # If no exact match, try partial matches\n",
    "    for opt in options:\n",
    "        # Check if the option is contained within the chosen answer\n",
    "        if opt.lower().strip() in chosen_option.lower():\n",
    "            matches.append(opt)\n",
    "    \n",
    "    # If we found matches, return the first one\n",
    "    if matches:\n",
    "        return [matches[0]], explanation\n",
    "    \n",
    "    # If no matches but we have a chosen_option, return it\n",
    "    if chosen_option and len(chosen_option) > 1:\n",
    "        return [chosen_option], explanation\n",
    "    \n",
    "    return [\"No valid answer extracted\"], explanation\n",
    "\n",
    "def evaluate_answer(correct_answer: str, claude_answer: List[str]) -> bool:\n",
    "    correct_answers = set(answer.strip().lower() for answer in correct_answer.split(','))\n",
    "    claude_answers = set(answer.strip().lower() for answer in claude_answer)\n",
    "    return bool(correct_answers & claude_answers)\n",
    "\n",
    "def evaluate_visualization_literacy(test_name: str, questions: List[Dict], prompt: str,\n",
    "                                 randomize_options: bool = False,\n",
    "                                 randomize_questions: bool = False) -> List[Dict]:\n",
    "    results = []\n",
    "    working_questions = questions.copy()\n",
    "    \n",
    "    if randomize_questions:\n",
    "        random.shuffle(working_questions)\n",
    "    \n",
    "    for idx, question in enumerate(working_questions, 1):\n",
    "        print(f\"\\nProcessing {test_name} question {idx}/{len(working_questions)}\")\n",
    "        print(f\"Conditions: Options {'Randomized' if randomize_options else 'Not Randomized'}, \"\n",
    "              f\"Questions {'Randomized' if randomize_questions else 'Not Randomized'}\")\n",
    "        \n",
    "        options = question['options'].copy()\n",
    "        if randomize_options:\n",
    "            random.shuffle(options)\n",
    "        \n",
    "        question_prompt = f\"Question: {question['question']}\\n\\nOptions:\\n\"\n",
    "        for i, option in enumerate(options, 1):\n",
    "            question_prompt += f\"{i}) {option}\\n\"\n",
    "        question_prompt += f\"\\n{prompt}\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        full_claude_answer = query_claude_with_retry(question_prompt, question['image_path'])\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "        print(f\"Raw Claude response: {full_claude_answer}\")\n",
    "        \n",
    "        claude_answer, explanation = extract_answer_and_explanation(full_claude_answer, options)\n",
    "        is_correct = evaluate_answer(question['correct_answer'], claude_answer)\n",
    "        \n",
    "        results.append({\n",
    "            'test_name': test_name,\n",
    "            'question': question['question'],\n",
    "            'options': ', '.join(options),\n",
    "            'correct_answer': question['correct_answer'],\n",
    "            'claude_answer': ', '.join(claude_answer),\n",
    "            'explanation': explanation,\n",
    "            'raw_response': full_claude_answer,\n",
    "            'Task': question.get('Task', ''),\n",
    "            'Chart_type': question.get('Chart_type', ''),\n",
    "            'Misleader': question.get('Misleader', ''),\n",
    "            'wrong_due_to_misleader': question.get('wrong_due_to_misleader', ''),\n",
    "            'is_correct': is_correct,\n",
    "            'randomized_options': randomize_options,\n",
    "            'randomized_questions': randomize_questions,\n",
    "            'image_path': question['image_path'],\n",
    "            'time_taken': time_taken\n",
    "        })\n",
    "        \n",
    "        print(f\"Question: {question['question']}\")\n",
    "        print(f\"Options: {', '.join(options)}\")\n",
    "        print(f\"Claude's answer: {', '.join(claude_answer)}\")\n",
    "        print(f\"Claude's explanation: {explanation}\")\n",
    "        print(f\"Correct answer: {question['correct_answer']}\")\n",
    "        print(f\"Result: {'Correct' if is_correct else 'Incorrect'}\")\n",
    "        \n",
    "        # Add a small delay between questions\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_experiment(test_name: str, file_path: str, prompt: str):\n",
    "    print(f\"\\nStarting {test_name} experiment...\")\n",
    "    questions = load_questions(file_path)\n",
    "    \n",
    "    conditions = [\n",
    "        (False, False, \"No_Randomization\"),\n",
    "        (True, False, \"Randomized_Options\"),\n",
    "        (False, True, \"Randomized_Questions\"),\n",
    "        (True, True, \"Both_Randomized\")\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for randomize_options, randomize_questions, condition_name in conditions:\n",
    "        print(f\"\\n=== Running {test_name} - {condition_name} ===\")\n",
    "        \n",
    "        for run in range(1, NUM_RUNS + 1):\n",
    "            print(f\"\\n--- Run {run}/{NUM_RUNS} ---\")\n",
    "            try:\n",
    "                results = evaluate_visualization_literacy(\n",
    "                    test_name,\n",
    "                    questions, \n",
    "                    prompt,\n",
    "                    randomize_options=randomize_options,\n",
    "                    randomize_questions=randomize_questions\n",
    "                )\n",
    "                \n",
    "                # Add condition information to results\n",
    "                for result in results:\n",
    "                    result['condition'] = condition_name\n",
    "                    result['run'] = run\n",
    "                \n",
    "                all_results.extend(results)\n",
    "                \n",
    "                # Save individual run results\n",
    "                df_run = pd.DataFrame(results)\n",
    "                output_file = f'claude_{test_name.lower()}_{condition_name}_run_{run}.csv'\n",
    "                df_run.to_csv(output_file, index=False)\n",
    "                \n",
    "                # Calculate and print run score\n",
    "                score = (df_run['is_correct'].sum() / len(df_run)) * 100\n",
    "                avg_time = df_run['time_taken'].mean()\n",
    "                print(f\"\\nScore for {condition_name} Run {run}: {score:.2f}%\")\n",
    "                print(f\"Average time per question: {avg_time:.2f} seconds\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        combined_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Calculate and print overall statistics\n",
    "        print(\"\\n=== Overall Results ===\")\n",
    "        for condition in combined_df['condition'].unique():\n",
    "            condition_df = combined_df[combined_df['condition'] == condition]\n",
    "            print(f\"\\n{condition}:\")\n",
    "            print(f\"Mean accuracy: {condition_df['is_correct'].mean() * 100:.2f}%\")\n",
    "            print(f\"Best question accuracy: {condition_df.groupby('question')['is_correct'].mean().max() * 100:.2f}%\")\n",
    "            print(f\"Worst question accuracy: {condition_df.groupby('question')['is_correct'].mean().min() * 100:.2f}%\")\n",
    "            print(f\"Average time per question: {condition_df['time_taken'].mean():.2f} seconds\")\n",
    "            print(f\"Fastest question: {condition_df['time_taken'].min():.2f} seconds\")\n",
    "            print(f\"Slowest question: {condition_df['time_taken'].max():.2f} seconds\")\n",
    "        \n",
    "        # Statistics by various dimensions\n",
    "        print(\"\\nAverage Statistics by Task:\")\n",
    "        print(combined_df.groupby(['Task', 'condition'])['is_correct'].mean().unstack())\n",
    "        \n",
    "        print(\"\\nAverage Time by Task:\")\n",
    "        print(combined_df.groupby(['Task', 'condition'])['time_taken'].mean().unstack())\n",
    "        \n",
    "        print(\"\\nAverage Statistics by Chart Type:\")\n",
    "        print(combined_df.groupby(['Chart_type', 'condition'])['is_correct'].mean().unstack())\n",
    "        \n",
    "        print(\"\\nAverage Time by Chart Type:\")\n",
    "        print(combined_df.groupby(['Chart_type', 'condition'])['time_taken'].mean().unstack())\n",
    "        \n",
    "        if 'Misleader' in combined_df.columns and not combined_df['Misleader'].isna().all():\n",
    "            print(\"\\nAverage Statistics by Misleader Type:\")\n",
    "            print(combined_df.groupby(['Misleader', 'condition'])['is_correct'].mean().unstack())\n",
    "            \n",
    "            misleader_stats = combined_df[combined_df['wrong_due_to_misleader'].notna()]\n",
    "            if not misleader_stats.empty:\n",
    "                print(\"\\nAccuracy for Questions with Misleader Issues:\")\n",
    "                print(misleader_stats.groupby(['wrong_due_to_misleader', 'condition'])['is_correct'].mean().unstack())\n",
    "        \n",
    "        # Save combined results\n",
    "        output_file = f'claude_{test_name.lower()}_all_results.csv'\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nCombined results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2faf95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_experiment(\"VLAT\", \"vlat_skip.json\", VLAT_PROMPT)\n",
    "        run_experiment(\"CALVI\", \"calvi.json\", CALVI_PROMPT)\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0941dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f99f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
