{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import mimetypes\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ba513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_RUNS = 10\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2\n",
    "API_URL = \"http://localhost:11434/api/chat\"\n",
    "TEMPERATURE = 0\n",
    "MAX_TOKENS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a39277",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLAT_PROMPT = \"\"\"I am about to show you an image and ask you a multiple choice question about that image. \n",
    "Please structure your response in the following format:\n",
    "\n",
    "Answer: [Enter the exact text of your chosen option]\n",
    "Explanation: [Provide your reasoning]\n",
    "\n",
    "Select the BEST answer, based only on the chart and not external knowledge. DO NOT GUESS.\n",
    "If you are not sure about your answer or your answer is based on a guess, select \"Omit\".\n",
    "Choose your answer ONLY from the provided options.\"\"\"\n",
    "\n",
    "CALVI_PROMPT = \"\"\"I am about to show you an image and ask you a multiple choice question about that image. \n",
    "Please structure your response in the following format:\n",
    "\n",
    "Answer: [Enter the exact text of your chosen option(s)]\n",
    "Explanation: [Provide your reasoning]\n",
    "\n",
    "Select the BEST answer, based only on the chart and not external knowledge.\n",
    "Choose your answer ONLY from the provided options.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f877a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data['questions']\n",
    "\n",
    "def encode_image(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image {image_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def query_llama_with_retry(prompt, image_path):\n",
    "    base64_image = encode_image(image_path)\n",
    "    mime_type = mimetypes.guess_type(image_path)[0] or 'image/png'\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            data = {\n",
    "                \"model\": \"llama3.2-vision\",\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                    \"images\": [base64_image]\n",
    "                }],\n",
    "                \"stream\": False,\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"max_tokens\": MAX_TOKENS\n",
    "            }\n",
    "            \n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            response = requests.post(API_URL, headers=headers, json=data)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()[\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                print(f\"Error: Received status code {response.status_code}\")\n",
    "                print(f\"Response content: {response.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred (Attempt {attempt + 1}/{MAX_RETRIES}): {str(e)}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this question.\")\n",
    "                return \"Error: Max retries reached\"\n",
    "\n",
    "\n",
    "def extract_answer_and_explanation(model_answer: str, options: List[str]) -> tuple:\n",
    "    \"\"\"Extract the answer and explanation from the model's response\"\"\"\n",
    "    # First try to find the last instance of \"Answer:\" in the text\n",
    "    answer_marker = \"Answer:\"\n",
    "    explanation_marker = \"Explanation:\"\n",
    "    \n",
    "    try:\n",
    "        # Split into answer and explanation parts\n",
    "        if answer_marker in model_answer:\n",
    "            # Find the last occurrence of Answer: as it's likely the final answer\n",
    "            last_answer_idx = model_answer.rindex(answer_marker)\n",
    "            remaining_text = model_answer[last_answer_idx:]\n",
    "            \n",
    "            # Split answer from explanation if explanation marker exists\n",
    "            if explanation_marker in remaining_text:\n",
    "                parts = remaining_text.split(explanation_marker)\n",
    "                answer_part = parts[0].split(answer_marker)[1].strip()\n",
    "                explanation = parts[1].strip()\n",
    "            else:\n",
    "                # If no explanation marker, take everything after the last answer marker\n",
    "                # and look for a natural break (newline or period)\n",
    "                answer_text = remaining_text.split(answer_marker)[1].strip()\n",
    "                # Try to split on newline first\n",
    "                if '\\n' in answer_text:\n",
    "                    answer_parts = answer_text.split('\\n', 1)\n",
    "                    answer_part = answer_parts[0].strip()\n",
    "                    explanation = answer_parts[1].strip()\n",
    "                else:\n",
    "                    # Try to split on period\n",
    "                    answer_parts = answer_text.split('.', 1)\n",
    "                    answer_part = answer_parts[0].strip()\n",
    "                    explanation = answer_parts[1].strip() if len(answer_parts) > 1 else \"No explanation provided\"\n",
    "            \n",
    "            # Clean up the answer\n",
    "            answer_part = answer_part.strip('*').strip()\n",
    "            \n",
    "            # Try to extract option number if present\n",
    "            option_number_match = re.search(r'(\\d+)\\)', answer_part)\n",
    "            if option_number_match:\n",
    "                option_number = int(option_number_match.group(1))\n",
    "                if 1 <= option_number <= len(options):\n",
    "                    return [options[option_number-1]], explanation\n",
    "            \n",
    "            # Direct matching with options\n",
    "            matches = []\n",
    "            for opt in options:\n",
    "                if opt in answer_part:\n",
    "                    matches.append(opt)\n",
    "            \n",
    "            # If no direct matches, try case-insensitive matching\n",
    "            if not matches:\n",
    "                for opt in options:\n",
    "                    if opt.lower() in answer_part.lower():\n",
    "                        matches.append(opt)\n",
    "            \n",
    "            # Special case for \"Omit\"\n",
    "            if not matches and \"omit\" in answer_part.lower() and \"Omit\" in options:\n",
    "                matches.append(\"Omit\")\n",
    "            \n",
    "            if matches:\n",
    "                return matches, explanation\n",
    "                \n",
    "        # Fallback to legacy parsing if no Answer: marker or no matches found\n",
    "        return legacy_extract_answer_and_explanation(model_answer, options)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {str(e)}\")\n",
    "        return legacy_extract_answer_and_explanation(model_answer, options)\n",
    "\n",
    "def legacy_extract_answer_and_explanation(model_answer: str, options: List[str]) -> tuple:\n",
    "    \"\"\"Legacy method for extracting answers when structured format isn't followed\"\"\"\n",
    "    # Search for patterns like \"answer: X\" or \"X) option\" anywhere in the text\n",
    "    answer_patterns = [\n",
    "        r'answer:\\s*([^.\\n]+)',\n",
    "        r'(?:^|\\n)\\s*(\\d+\\)[\\s\\w\\d\\-\\.,$]+)',\n",
    "        r'the correct answer is:?\\s*([^.\\n]+)',\n",
    "        r'therefore,?\\s*(?:the correct answer is:?)?\\s*([^.\\n]+)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in answer_patterns:\n",
    "        matches = re.finditer(pattern, model_answer.lower(), re.MULTILINE)\n",
    "        for match in matches:\n",
    "            potential_answer = match.group(1).strip()\n",
    "            # Check if this potential answer contains any of our options\n",
    "            for opt in options:\n",
    "                if opt.lower() in potential_answer:\n",
    "                    # Get the text after this match as the explanation\n",
    "                    explanation_start = match.end()\n",
    "                    explanation = model_answer[explanation_start:].strip()\n",
    "                    if not explanation:\n",
    "                        explanation = \"No explanation provided\"\n",
    "                    return [opt], explanation\n",
    "            \n",
    "            # Check for option numbers\n",
    "            number_match = re.search(r'(\\d+)\\)', potential_answer)\n",
    "            if number_match:\n",
    "                option_num = int(number_match.group(1))\n",
    "                if 1 <= option_num <= len(options):\n",
    "                    explanation_start = match.end()\n",
    "                    explanation = model_answer[explanation_start:].strip()\n",
    "                    if not explanation:\n",
    "                        explanation = \"No explanation provided\"\n",
    "                    return [options[option_num-1]], explanation\n",
    "    \n",
    "    # If no patterns matched, fall back to the original split logic\n",
    "    separators = ['why:', 'because', 'as', 'since', 'explanation:', 'reasoning:', ':']\n",
    "    parts = None\n",
    "    \n",
    "    for sep in separators:\n",
    "        if sep in model_answer.lower():\n",
    "            parts = model_answer.lower().split(sep, 1)\n",
    "            if len(parts) > 1:\n",
    "                break\n",
    "    \n",
    "    if not parts:\n",
    "        parts = [model_answer, \"No explanation provided\"]\n",
    "    \n",
    "    chosen_option = parts[0].strip()\n",
    "    explanation = parts[1].strip() if len(parts) > 1 else \"No explanation provided\"\n",
    "    \n",
    "    matches = []\n",
    "    for opt in options:\n",
    "        if opt.lower() in chosen_option.lower():\n",
    "            matches.append(opt)\n",
    "    \n",
    "    if not matches:\n",
    "        # Try to find the last occurrence of a number followed by )\n",
    "        numbers = re.findall(r'(\\d+)\\)', chosen_option)\n",
    "        if numbers:\n",
    "            last_num = int(numbers[-1])  # Take the last number found\n",
    "            if 1 <= last_num <= len(options):\n",
    "                return [options[last_num-1]], explanation\n",
    "    \n",
    "    if not matches:\n",
    "        print(f\"Warning: Could not match response '{chosen_option}' to any option exactly.\")\n",
    "        print(f\"Available options were: {options}\")\n",
    "        # Return the most likely option based on the last mentioned answer\n",
    "        if \"therefore\" in model_answer.lower():\n",
    "            last_part = model_answer.lower().split(\"therefore\")[-1]\n",
    "            for opt in options:\n",
    "                if opt.lower() in last_part:\n",
    "                    return [opt], explanation\n",
    "    \n",
    "    return matches if matches else [chosen_option], explanation\n",
    "\n",
    "def evaluate_answer(correct_answer: str, model_answers: List[str]) -> bool:\n",
    "    \"\"\"Evaluate if the model's answer matches the correct answer\"\"\"\n",
    "    # Convert answers to sets for comparison\n",
    "    correct_set = {ans.strip() for ans in correct_answer.split(',')}\n",
    "    model_set = {ans.strip() for ans in model_answers}\n",
    "    \n",
    "    # First try exact matching\n",
    "    if correct_set & model_set:\n",
    "        return True\n",
    "        \n",
    "    # If no exact match, try case-insensitive matching\n",
    "    correct_lower = {ans.lower() for ans in correct_set}\n",
    "    model_lower = {ans.lower() for ans in model_set}\n",
    "    \n",
    "    return bool(correct_lower & model_lower)\n",
    "\n",
    "def evaluate_visualization_literacy(test_name: str, questions: list, prompt: str,\n",
    "                                 randomize_options: bool = False,\n",
    "                                 randomize_questions: bool = False) -> list:\n",
    "    results = []\n",
    "    working_questions = questions.copy()\n",
    "    \n",
    "    if randomize_questions:\n",
    "        random.shuffle(working_questions)\n",
    "    \n",
    "    for idx, question in enumerate(working_questions, 1):\n",
    "        print(f\"\\nProcessing {test_name} question {idx}/{len(working_questions)}\")\n",
    "        print(f\"Conditions: Options {'Randomized' if randomize_options else 'Not Randomized'}, \"\n",
    "              f\"Questions {'Randomized' if randomize_questions else 'Not Randomized'}\")\n",
    "        \n",
    "        options = question['options'].copy()\n",
    "        if randomize_options:\n",
    "            random.shuffle(options)\n",
    "        \n",
    "        # Format multiple choice question\n",
    "        question_prompt = f\"Question: {question['question']}\\n\\nOptions:\\n\"\n",
    "        for i, option in enumerate(options, 1):\n",
    "            question_prompt += f\"{i}) {option}\\n\"\n",
    "        question_prompt += f\"\\n{prompt}\"  # Add the test-specific prompt\n",
    "        \n",
    "        start_time = time.time()\n",
    "        full_llama_answer = query_llama_with_retry(question_prompt, question['image_path'])\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "        print(f\"Raw Llama response: {full_llama_answer}\")\n",
    "        \n",
    "        llama_answer, explanation = extract_answer_and_explanation(full_llama_answer, options)\n",
    "        is_correct = evaluate_answer(question['correct_answer'], llama_answer)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'test_name': test_name,\n",
    "            'question': question['question'],\n",
    "            'options': ', '.join(options),\n",
    "            'correct_answer': question['correct_answer'],\n",
    "            'llama_answer': ', '.join(llama_answer),\n",
    "            'explanation': explanation,\n",
    "            'raw_response': full_llama_answer,\n",
    "            'Task': question['Task'],\n",
    "            'Chart_type': question['Chart_type'],\n",
    "            'Misleader': question.get('Misleader', ''),\n",
    "            'wrong_due_to_misleader': question.get('wrong_due_to_misleader', ''),\n",
    "            'is_correct': is_correct,\n",
    "            'randomized_options': randomize_options,\n",
    "            'randomized_questions': randomize_questions,\n",
    "            'image_path': question['image_path'],\n",
    "            'time_taken': time_taken\n",
    "        })\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Question: {question['question']}\")\n",
    "        print(f\"Options: {', '.join(options)}\")\n",
    "        print(f\"Llama's answer: {', '.join(llama_answer)}\")\n",
    "        print(f\"Llama's explanation: {explanation}\")\n",
    "        print(f\"Correct answer: {question['correct_answer']}\")\n",
    "        print(f\"Result: {'Correct' if is_correct else 'Incorrect'}\")\n",
    "        \n",
    "        # Add a small delay between questions\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_experiment(test_name: str, file_path: str, prompt: str):\n",
    "    print(f\"\\nStarting {test_name} experiment...\")\n",
    "    questions = load_questions(file_path)\n",
    "    \n",
    "    conditions = [\n",
    "         (False, False, \"No_Randomization\"),\n",
    "        (True, False, \"Randomized_Options\"),\n",
    "        (False, True, \"Randomized_Questions\"),\n",
    "        (True, True, \"Both_Randomized\")\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for randomize_options, randomize_questions, condition_name in conditions:\n",
    "        print(f\"\\n=== Running {test_name} - {condition_name} ===\")\n",
    "        \n",
    "        for run in range(1, NUM_RUNS + 1):\n",
    "            print(f\"\\n--- Run {run}/{NUM_RUNS} ---\")\n",
    "            try:\n",
    "                results = evaluate_visualization_literacy(\n",
    "                    test_name,\n",
    "                    questions, \n",
    "                    prompt,\n",
    "                    randomize_options=randomize_options,\n",
    "                    randomize_questions=randomize_questions\n",
    "                )\n",
    "                \n",
    "                # Add condition information to results\n",
    "                for result in results:\n",
    "                    result['condition'] = condition_name\n",
    "                    result['run'] = run\n",
    "                \n",
    "                all_results.extend(results)\n",
    "                \n",
    "                # Save individual run results\n",
    "                df_run = pd.DataFrame(results)\n",
    "                output_file = f'llama_vision_{test_name.lower()}_{condition_name}_run_{run}.csv'\n",
    "                df_run.to_csv(output_file, index=False)\n",
    "                \n",
    "                # Calculate and print run score\n",
    "                score = (df_run['is_correct'].sum() / len(df_run)) * 100\n",
    "                avg_time = df_run['time_taken'].mean()\n",
    "                print(f\"\\nScore for {condition_name} Run {run}: {score:.2f}%\")\n",
    "                print(f\"Average time per question: {avg_time:.2f} seconds\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        combined_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Calculate and print overall statistics\n",
    "        print(\"\\n=== Overall Results ===\")\n",
    "        for condition in combined_df['condition'].unique():\n",
    "            condition_df = combined_df[combined_df['condition'] == condition]\n",
    "            print(f\"\\n{condition}:\")\n",
    "            print(f\"Mean accuracy: {condition_df['is_correct'].mean() * 100:.2f}%\")\n",
    "            print(f\"Best question accuracy: {condition_df.groupby('question')['is_correct'].mean().max() * 100:.2f}%\")\n",
    "            print(f\"Worst question accuracy: {condition_df.groupby('question')['is_correct'].mean().min() * 100:.2f}%\")\n",
    "            print(f\"Average time per question: {condition_df['time_taken'].mean():.2f} seconds\")\n",
    "            print(f\"Fastest question: {condition_df['time_taken'].min():.2f} seconds\")\n",
    "            print(f\"Slowest question: {condition_df['time_taken'].max():.2f} seconds\")\n",
    "        \n",
    "        # Statistics by various dimensions\n",
    "        print(\"\\nAverage Statistics by Task:\")\n",
    "        print(combined_df.groupby(['Task', 'condition'])['is_correct'].mean().unstack())\n",
    "        \n",
    "        print(\"\\nAverage Time by Task:\")\n",
    "        print(combined_df.groupby(['Task', 'condition'])['time_taken'].mean().unstack())\n",
    "        \n",
    "        print(\"\\nAverage Statistics by Chart Type:\")\n",
    "        print(combined_df.groupby(['Chart_type', 'condition'])['is_correct'].mean().unstack())\n",
    "        \n",
    "        print(\"\\nAverage Time by Chart Type:\")\n",
    "        print(combined_df.groupby(['Chart_type', 'condition'])['time_taken'].mean().unstack())\n",
    "        \n",
    "        if 'Misleader' in combined_df.columns:\n",
    "            print(\"\\nAverage Statistics by Misleader Type:\")\n",
    "            print(combined_df.groupby(['Misleader', 'condition'])['is_correct'].mean().unstack())\n",
    "        \n",
    "        # Save combined results\n",
    "        output_file = f'llama_vision_{test_name.lower()}_all_results.csv'\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nCombined results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af81a7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_experiment(\"VLAT\", \"vlat_skip.json\", VLAT_PROMPT)\n",
    "        run_experiment(\"CALVI\", \"calvi.json\", CALVI_PROMPT)\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d0758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a52bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
